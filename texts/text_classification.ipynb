{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import numpy as np\n",
    "import random\n",
    "import PySide6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25474 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"text_data/text_data_train.csv\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# separate into texts (paragraphs) and labels (authorship)\n",
    "texts = list(df['text'])\n",
    "labels = list(df['author_is_TW'])\n",
    "\n",
    "# parameters\n",
    "maxlen = 250\n",
    "\n",
    "# train on 5000 samples\n",
    "training_samples = 5000\n",
    "\n",
    "# validate on 2000 samples\n",
    "validation_samples = 2000\n",
    "\n",
    "# consider only the top 10k words in the dataset\n",
    "max_words = 10000\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# helper function to add spaces before common punctuation, so that these symbols will be read as their own tokens\n",
    "def separate_punctuation(txts):\n",
    "    for t in range(len(txts)):\n",
    "        txts[t] = txts[t].replace(\".\", \" .\")\n",
    "        txts[t] = txts[t].replace(\"!\", \" !\")\n",
    "        txts[t] = txts[t].replace(\"?\", \" ?\")\n",
    "        txts[t] = txts[t].replace(\":\", \" :\")\n",
    "        txts[t] = txts[t].replace(\";\", \" ;\")\n",
    "        txts[t] = txts[t].replace(\",\", \" ,\")\n",
    "        txts[t] = txts[t].replace(\"(\", \"( \")\n",
    "        txts[t] = txts[t].replace(\")\", \" )\")\n",
    "        txts[t] = txts[t].replace('\"', ' \" ')\n",
    "    return txts\n",
    "\n",
    "\n",
    "texts = separate_punctuation(texts)\n",
    "\n",
    "# tokenize using keras built-in tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index   # save index mapping numbers to words\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# pad sequences so that each sequence has the same length\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# convert labels to numpy array\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# training data\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "# validation data\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# add an embedding layer\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "# flatten the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen*embedding_dim)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.3707 - acc: 0.8246 - precision: 0.8228 - recall: 0.8942 - val_loss: 0.1534 - val_acc: 0.9410 - val_precision: 0.9087 - val_recall: 0.9937\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.0457 - acc: 0.9866 - precision: 0.9855 - recall: 0.9918 - val_loss: 0.1236 - val_acc: 0.9495 - val_precision: 0.9711 - val_recall: 0.9371\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0082 - acc: 0.9976 - precision: 0.9980 - recall: 0.9980 - val_loss: 0.1255 - val_acc: 0.9585 - val_precision: 0.9486 - val_recall: 0.9784\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0012 - acc: 0.9996 - precision: 0.9997 - recall: 0.9997 - val_loss: 0.1466 - val_acc: 0.9610 - val_precision: 0.9641 - val_recall: 0.9658\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 3.1000e-04 - acc: 0.9998 - precision: 0.9997 - recall: 1.0000 - val_loss: 0.1812 - val_acc: 0.9580 - val_precision: 0.9639 - val_recall: 0.9604\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.9169e-05 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2173 - val_acc: 0.9575 - val_precision: 0.9589 - val_recall: 0.9649\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 1.1534e-05 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2626 - val_acc: 0.9530 - val_precision: 0.9545 - val_recall: 0.9613\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 7.5460e-07 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2725 - val_acc: 0.9525 - val_precision: 0.9536 - val_recall: 0.9613\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.7664e-07 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2909 - val_acc: 0.9555 - val_precision: 0.9555 - val_recall: 0.9649\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 4.5367e-08 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.3041 - val_acc: 0.9520 - val_precision: 0.9496 - val_recall: 0.9649\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in holdout data\n",
    "df_holdout = pd.read_csv(\"text_data/text_data_holdout.csv\")\n",
    "texts_holdout = list(df_holdout['text'])\n",
    "labels_holdout = list(df_holdout['author_is_TW'])\n",
    "\n",
    "# separate punctuation\n",
    "texts_holdout = separate_punctuation(texts_holdout)\n",
    "\n",
    "# convert to sequences\n",
    "holdout_sequences = tokenizer.texts_to_sequences(texts_holdout)\n",
    "\n",
    "# \"pad\" sequences so that each sequence has the same length\n",
    "holdout_data = pad_sequences(holdout_sequences, maxlen=maxlen)\n",
    "\n",
    "# convert labels (list) to a numpy array\n",
    "holdout_labels = np.asarray(labels_holdout)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(holdout_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_holdout = holdout_data[indices]\n",
    "y_holdout = holdout_labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: [\"the editor of the local paper heard of it and sent a reporter to interview him and printed a story about it so you've written a book said the reporter what kind of a book is it what's it about why i i hardly know how to tell you george stammered\"]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "predicted label: 1.0\n",
      "actual label: 1\n",
      "text: ['who are you he said hoarsely holding a hairy hand carefully beside his mouth prince hal said eugene likewise hoarsely and behind his hand']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "predicted label: 1.0\n",
      "actual label: 1\n",
      "text: [\"these jews she cried such things would never happen if it were not for them they make all the trouble germany has had to protect herself the jews were taking all the money from the country thousands of them escaped taking millions of marks with them and now when it's too late we wake up to it it's too bad that foreigners must see these things that they've got to go through these painful experiences it makes a bad impression they don't understand the reason but it's the jews she whispered\"]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "predicted label: 1.0\n",
      "actual label: 1\n",
      "text: ['down the coiling road that came through the gap from the town a glinted through the trees two men were in it his face hardened against it he watched it go by his gates on the road with a of dust dimly he saw their lewd red mountain faces and completed the image with sweat and and in the town their city brick the white little of half of the world']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "predicted label: 0.99\n",
      "actual label: 1\n",
      "text: [\"as george went into the washroom suddenly be came upon the mayor cleaning his false teeth in the basin the man's plump face which george had always known in the of cheerful hearty was all caved in hearing a sound behind him the mayor turned upon the for a moment there was nothing but nameless fright in his weak brown eyes he mumbled frantically holding his false teeth in his trembling fingers like a man who did not know what he was doing he brandished them in a grotesque yet terrible gesture of god knows what but despair and terror were both in it then he put the teeth into his mouth again smiled feebly and muttered apologetically with some counterfeit of his usual ho ho well son\"]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "predicted label: 1.0\n",
      "actual label: 1\n"
     ]
    }
   ],
   "source": [
    "# Display sample predictions\n",
    "for i in range(5):\n",
    "    indx = random.randint(0, len(x_holdout))\n",
    "    print('text:', tokenizer.sequences_to_texts([x_holdout[indx]]))\n",
    "    print('predicted label:', round(model.predict(np.array([x_holdout[indx]]))[0][0], 2))\n",
    "    print('actual label:', y_holdout[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2253 - acc: 0.9606 - precision: 0.9625 - recall: 0.9707\n",
      "loss on hold-out data: 0.2253234088420868\n",
      "accuracy on hold-out data: 0.9605539441108704\n"
     ]
    }
   ],
   "source": [
    "# Calculate model accuracy and loss\n",
    "\n",
    "holdout_results = model.evaluate(x_holdout, y_holdout)\n",
    "print('loss on hold-out data:', holdout_results[0])\n",
    "print('accuracy on hold-out data:', holdout_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25616 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# re-import data\n",
    "df = pd.read_csv(\"text_data/text_data_train.csv\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# separate into texts (paragraphs) and labels (authorship)\n",
    "texts = list(df['text'])\n",
    "labels = list(df['author_is_TW'])\n",
    "\n",
    "# cut off paragraphs after 300 words\n",
    "maxlen = 300\n",
    "\n",
    "# consider the top 20k words in the dataset\n",
    "max_words = 20000\n",
    "\n",
    "# set higher embedding dimension (150)\n",
    "embedding_dim = 150\n",
    "\n",
    "# # tokenize the text and convert to numerical sequences\n",
    "# tokenize using keras built-in tokenizer, but this time don't filter out punctuation (potentially informative)\n",
    "\n",
    "# helper function to add spaces before common punctuation, so that these symbols will be read as their own tokens\n",
    "def separate_punctuation(txts):\n",
    "    for t in range(len(txts)):\n",
    "        txts[t] = txts[t].replace(\".\", \" .\")\n",
    "        txts[t] = txts[t].replace(\"!\", \" !\")\n",
    "        txts[t] = txts[t].replace(\"?\", \" ?\")\n",
    "        txts[t] = txts[t].replace(\":\", \" :\")\n",
    "        txts[t] = txts[t].replace(\";\", \" ;\")\n",
    "        txts[t] = txts[t].replace(\",\", \" ,\")\n",
    "        txts[t] = txts[t].replace(\"(\", \"( \")\n",
    "        txts[t] = txts[t].replace(\")\", \" )\")\n",
    "        txts[t] = txts[t].replace('\"', ' \" ')\n",
    "    return txts\n",
    "\n",
    "\n",
    "texts = separate_punctuation(texts)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words,\n",
    "                      filters='#$%*+-/<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "# tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index   # save index mapping numbers to words\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# \"pad\" sequences so that each sequence has the same length\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# convert labels (list) to a numpy array\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# training data\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "# validation data\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "157/157 [==============================] - 13s 75ms/step - loss: 0.3622 - acc: 0.8514 - precision: 0.8285 - recall: 0.9366 - val_loss: 0.1535 - val_acc: 0.9530 - val_precision: 0.9759 - val_recall: 0.9422\n",
      "Epoch 2/25\n",
      "157/157 [==============================] - 12s 75ms/step - loss: 0.0729 - acc: 0.9812 - precision: 0.9847 - recall: 0.9827 - val_loss: 0.1127 - val_acc: 0.9590 - val_precision: 0.9847 - val_recall: 0.9439\n",
      "Epoch 3/25\n",
      "157/157 [==============================] - 12s 74ms/step - loss: 0.0214 - acc: 0.9964 - precision: 0.9962 - recall: 0.9976 - val_loss: 0.0852 - val_acc: 0.9730 - val_precision: 0.9694 - val_recall: 0.9845\n",
      "Epoch 4/25\n",
      "157/157 [==============================] - 12s 74ms/step - loss: 0.0143 - acc: 0.9968 - precision: 0.9972 - recall: 0.9972 - val_loss: 0.1246 - val_acc: 0.9545 - val_precision: 0.9794 - val_recall: 0.9413\n",
      "Epoch 5/25\n",
      "157/157 [==============================] - 12s 74ms/step - loss: 0.0071 - acc: 0.9982 - precision: 0.9990 - recall: 0.9979 - val_loss: 0.0936 - val_acc: 0.9700 - val_precision: 0.9701 - val_recall: 0.9784\n",
      "Epoch 6/25\n",
      "157/157 [==============================] - 12s 74ms/step - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.0988 - val_acc: 0.9720 - val_precision: 0.9678 - val_recall: 0.9845\n",
      "Epoch 7/25\n",
      "157/157 [==============================] - 12s 74ms/step - loss: 6.0829e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1103 - val_acc: 0.9720 - val_precision: 0.9710 - val_recall: 0.9810\n",
      "Epoch 8/25\n",
      "156/157 [============================>.] - ETA: 0s - loss: 4.3236e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000Restoring model weights from the end of the best epoch: 3.\n",
      "157/157 [==============================] - 12s 74ms/step - loss: 4.3210e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1293 - val_acc: 0.9710 - val_precision: 0.9725 - val_recall: 0.9776\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# initialize a sequential model\n",
    "model = Sequential()\n",
    "# add an embedding layer\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "# add dropout layers between each subsequent layer (to help slow overfitting by introducing random noise)\n",
    "model.add(Dropout(0.15))\n",
    "# use an LSTM (\"Long Short-Term Memory) layer rather than a Dense layer; this allows the model to have memory, turning\n",
    "# it into an \"RNN\" (Recurrent Neural Network)... Another common layer type for this purpose is the \"GRU\" (Gated\n",
    "# Recurrent Unit)\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# compile model (now using Keras' Adam optimizer, widely considered the gold standard)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', 'Precision', 'Recall'])\n",
    "\n",
    "# add an early stopping rule for training\n",
    "# this will end training early if the chosen monitor (here, validation accuracy) stops improving\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  patience=5,\n",
    "                                                  mode='max',\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['“how are yours ?” .” “let’s see them .” “they’re packed .” “how big are they really ?” “they’re all about the size of your smallest .” “you’re not holding out on me ?” “i wish i were .” “get them all on worms ?” “yes .” “you lazy bum !”']\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "predicted label: 0.0\n",
      "actual label: 0\n",
      "text: [\"'i been thinking a lot tonight ,' her dad said . he poured out his beer and sprinkled a few of salt on the back of his hand . then he licked up the salt and took a swallow out of the glass .\"]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "predicted label: 0.0\n",
      "actual label: 0\n",
      "text: ['it was a photograph of half a dozen young men in loafing in an through which were visible a host of spires . there was gatsby , looking a little , not much , a bat in his hand . then it was all true . i saw the skins of flaming in his palace on the grand canal ; i saw him opening a chest of rubies to ease , with their crimson lighted depths , the of his broken heart .']\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "predicted label: 0.09\n",
      "actual label: 0\n",
      "text: ['“it’s all right ,” i said . “i don’t give a damn any more .” “really ?” “really . only i’d a hell of a lot rather not talk about it .”']\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "predicted label: 0.0\n",
      "actual label: 0\n",
      "text: ['check this one off , \" said jennings ware , \" if you can\\'t collect next time . she for six weeks now . this one , \" he said , a paper quietly on a door mat , \" is good pay .']\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "predicted label: 1.0\n",
      "actual label: 1\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1017 - acc: 0.9710 - precision: 0.9684 - recall: 0.9829\n",
      "loss on hold-out data: 0.10169907659292221\n",
      "accuracy on hold-out data: 0.9710448980331421\n"
     ]
    }
   ],
   "source": [
    "# read in holdout data\n",
    "df_holdout = pd.read_csv(\"text_data/text_data_holdout.csv\")\n",
    "texts_holdout = list(df_holdout['text'])\n",
    "labels_holdout = list(df_holdout['author_is_TW'])\n",
    "\n",
    "# separate punctuation\n",
    "texts_holdout = separate_punctuation(texts_holdout)\n",
    "\n",
    "# convert to sequences\n",
    "holdout_sequences = tokenizer.texts_to_sequences(texts_holdout)\n",
    "\n",
    "# \"pad\" sequences so that each sequence has the same length\n",
    "holdout_data = pad_sequences(holdout_sequences, maxlen=maxlen)\n",
    "\n",
    "# convert labels (list) to a numpy array\n",
    "holdout_labels = np.asarray(labels_holdout)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(holdout_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_holdout = holdout_data[indices]\n",
    "y_holdout = holdout_labels[indices]\n",
    "\n",
    "# Show predictions for a random sample of holdout data\n",
    "for i in range(5):\n",
    "    indx = random.randint(0, len(x_holdout))\n",
    "    print('text:', tokenizer.sequences_to_texts([x_holdout[indx]]))\n",
    "    print('predicted label:', round(model.predict(np.array([x_holdout[indx]]))[0][0], 2))\n",
    "    print('actual label:', y_holdout[indx])\n",
    "\n",
    "# Evaluate model on the holdout data\n",
    "holdout_results = model.evaluate(x_holdout, y_holdout)\n",
    "print('loss on hold-out data:', holdout_results[0])\n",
    "print('accuracy on hold-out data:', holdout_results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
