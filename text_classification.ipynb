{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Qt5Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import PySide6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25474 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"texts/text_data_train.csv\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# separate into texts (paragraphs) and labels (authorship)\n",
    "texts = list(df['text'])\n",
    "labels = list(df['author_is_TW'])\n",
    "\n",
    "# parameters\n",
    "maxlen = 250\n",
    "\n",
    "# train on 5000 samples\n",
    "training_samples = 5000\n",
    "\n",
    "# validate on 2000 samples\n",
    "validation_samples = 2000\n",
    "\n",
    "# consider only the top 10k words in the dataset\n",
    "max_words = 10000\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# helper function to add spaces before common punctuation, so that these symbols will be read as their own tokens\n",
    "def separate_punctuation(txts):\n",
    "    for t in range(len(txts)):\n",
    "        txts[t] = txts[t].replace(\".\", \" .\")\n",
    "        txts[t] = txts[t].replace(\"!\", \" !\")\n",
    "        txts[t] = txts[t].replace(\"?\", \" ?\")\n",
    "        txts[t] = txts[t].replace(\":\", \" :\")\n",
    "        txts[t] = txts[t].replace(\";\", \" ;\")\n",
    "        txts[t] = txts[t].replace(\",\", \" ,\")\n",
    "        txts[t] = txts[t].replace(\"(\", \"( \")\n",
    "        txts[t] = txts[t].replace(\")\", \" )\")\n",
    "        txts[t] = txts[t].replace('\"', ' \" ')\n",
    "    return txts\n",
    "\n",
    "\n",
    "texts = separate_punctuation(texts)\n",
    "\n",
    "# tokenize using keras built-in tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index   # save index mapping numbers to words\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# pad sequences so that each sequence has the same length\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# convert labels to numpy array\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# training data\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "# validation data\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 250, 100)          1000000   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 25000)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                800032    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1800065 (6.87 MB)\n",
      "Trainable params: 1800065 (6.87 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# add an embedding layer\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "# flatten the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen*embedding_dim)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 2s 9ms/step - loss: 0.4050 - acc: 0.8160 - precision: 0.8041 - recall: 0.9028 - val_loss: 0.2861 - val_acc: 0.8715 - val_precision: 0.8186 - val_recall: 0.9991\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0683 - acc: 0.9800 - precision: 0.9762 - recall: 0.9897 - val_loss: 0.2628 - val_acc: 0.9035 - val_precision: 0.8575 - val_recall: 0.9991\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0195 - acc: 0.9934 - precision: 0.9924 - recall: 0.9962 - val_loss: 0.1475 - val_acc: 0.9360 - val_precision: 0.9840 - val_recall: 0.9040\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.0044 - acc: 0.9996 - precision: 0.9997 - recall: 0.9997 - val_loss: 0.1489 - val_acc: 0.9605 - val_precision: 0.9418 - val_recall: 0.9931\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.0016 - acc: 0.9994 - precision: 0.9993 - recall: 0.9997 - val_loss: 0.1607 - val_acc: 0.9640 - val_precision: 0.9578 - val_recall: 0.9810\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 2.4654e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1884 - val_acc: 0.9540 - val_precision: 0.9708 - val_recall: 0.9490\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 6.7495e-05 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1986 - val_acc: 0.9590 - val_precision: 0.9653 - val_recall: 0.9637\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.2874e-05 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2621 - val_acc: 0.9530 - val_precision: 0.9455 - val_recall: 0.9749\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 4.0212e-06 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2545 - val_acc: 0.9560 - val_precision: 0.9564 - val_recall: 0.9680\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 1.8795e-07 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2574 - val_acc: 0.9575 - val_precision: 0.9644 - val_recall: 0.9619\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in holdout data\n",
    "df_holdout = pd.read_csv(\"texts/text_data_holdout.csv\")\n",
    "texts_holdout = list(df_holdout['text'])\n",
    "labels_holdout = list(df_holdout['author_is_TW'])\n",
    "\n",
    "# separate punctuation\n",
    "texts_holdout = separate_punctuation(texts_holdout)\n",
    "\n",
    "# convert to sequences\n",
    "holdout_sequences = tokenizer.texts_to_sequences(texts_holdout)\n",
    "\n",
    "# \"pad\" sequences so that each sequence has the same length\n",
    "holdout_data = pad_sequences(holdout_sequences, maxlen=maxlen)\n",
    "\n",
    "# convert labels (list) to a numpy array\n",
    "holdout_labels = np.asarray(labels_holdout)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(holdout_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_holdout = holdout_data[indices]\n",
    "y_holdout = holdout_labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['happen at the ” “i didn’t see it all one man was badly ” “where ” “here ”']\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "predicted label: 0.0\n",
      "actual label: 0\n",
      "text: [\"and he would say gosh miss edith i didn't mean to do nothin' later as the golden sun was waning and there was nothing in the room but the smell of chalk and the heavy of the old october flies they would prepare to depart as he twisted carelessly into his overcoat she would him call him to her arrange the lapels and his necktie and smooth out his hair saying you're a good looking boy\"]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "predicted label: 0.99\n",
      "actual label: 1\n",
      "text: ['and now the city was left behind those familiar faces forms and voices of just six minutes past now seemed as remote as dreams imprisoned there as in another world a world of massive brick and stone and pavements a world of four million lives of hope and fear and hatred of anguish and despair of love of cruelty and devotion that was called berlin and now the land was stroking past the level land of the lonely of the north which he had always heard was so ugly and which he had found so strange so haunting and so beautiful the dark solitude of the forest was around them now the loneliness of the trees tall slender towering and as straight as sailing bearing upon their tops the slender burden of their and eternal green their naked poles shone with that lovely gold bronze colour which is like the material substance of a magic light and all between was magic too the forest dusk beneath the was gold brown also the earth gold brown and barren and the trees themselves stood alone and separate a forest filled with haunting light']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "predicted label: 1.0\n",
      "actual label: 1\n",
      "text: ['what are you trying to do with your kid mama he said in a hard quiet voice after a silence do you want to make a tramp out of him what do you mean']\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "predicted label: 1.0\n",
      "actual label: 1\n",
      "text: ['he lay there on the bed “well ” i said “i’m going to take a bath ” “you were the only friend i had and i loved brett so ”']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "predicted label: 0.0\n",
      "actual label: 0\n"
     ]
    }
   ],
   "source": [
    "# Display sample predictions\n",
    "for i in range(5):\n",
    "    indx = random.randint(0, len(x_holdout))\n",
    "    print('text:', tokenizer.sequences_to_texts([x_holdout[indx]]))\n",
    "    print('predicted label:', round(model.predict(np.array([x_holdout[indx]]))[0][0], 2))\n",
    "    print('actual label:', y_holdout[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2305 - acc: 0.9597 - precision: 0.9718 - recall: 0.9593\n",
      "loss on hold-out data: 0.2304934710264206\n",
      "accuracy on hold-out data: 0.9597146511077881\n"
     ]
    }
   ],
   "source": [
    "# Calculate model accuracy and loss\n",
    "\n",
    "holdout_results = model.evaluate(x_holdout, y_holdout)\n",
    "print('loss on hold-out data:', holdout_results[0])\n",
    "print('accuracy on hold-out data:', holdout_results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
